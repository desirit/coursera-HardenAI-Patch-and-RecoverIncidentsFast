# Prometheus Helm Chart Values
# Configures Prometheus to scrape model server metrics

server:
  persistentVolume:
    enabled: false  # Disable for lab (simpler setup)

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi

# Disable components we don't need for this lab
alertmanager:
  enabled: true
  persistentVolume:
    enabled: false

pushgateway:
  enabled: false

nodeExporter:
  enabled: false

kubeStateMetrics:
  enabled: false

# Prometheus server configuration
serverFiles:
  prometheus.yml:
    global:
      scrape_interval: 5s      # Scrape every 5 seconds
      evaluation_interval: 5s

    rule_files:
      - /etc/config/alerting_rules.yml

    scrape_configs:
      # Scrape Prometheus itself
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090

      # Scrape model server pods
      - job_name: 'model-server'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          # Only scrape pods with prometheus.io/scrape annotation
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          # Use prometheus.io/path annotation for metrics path
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          # Use prometheus.io/port annotation for port
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          # Add pod name label
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: pod
          # Add namespace label
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace

  # Alert rules for model health
  alerting_rules.yml:
    groups:
      - name: model-health
        rules:
          # Alert when model accuracy drops below 90%
          - alert: ModelAccuracyLow
            expr: model_accuracy < 0.90
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Model accuracy degraded"
              description: "Model accuracy is {{ $value | humanizePercentage }} (below 90%)"

          # Alert when model is in degraded state
          - alert: ModelDegraded
            expr: model_degraded == 1
            for: 10s
            labels:
              severity: critical
            annotations:
              summary: "Model is in degraded state"
              description: "Model version {{ $labels.version }} is degraded"

          # Alert when error rate is high
          - alert: ModelErrorRateHigh
            expr: model_error_rate > 0.1
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "Model error rate is high"
              description: "Error rate is {{ $value | humanizePercentage }}"

# Alertmanager configuration
alertmanagerFiles:
  alertmanager.yml:
    global:
      resolve_timeout: 1m

    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'default'

    receivers:
      - name: 'default'
        # In production, configure webhook, email, or Slack here
        # For this lab, alerts just show in Prometheus UI
